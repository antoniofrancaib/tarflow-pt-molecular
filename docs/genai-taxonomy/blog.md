# Generative Modeling and Optimal Transport: A Mathematical Framework

## 1. Introduction: The Generative Modeling Problem

Generative modeling field rests on a single mathematical foundation: **learning to sample from complex, high-dimensional probability distributions**. Whether the task is generating coherent text, synthesizing realistic images, or predicting protein structures, the underlying challenge is the same: given finite observations of some phenomenon, can we build a computational system that generates new, realistic samples that appear to come from the same distribution?

More formally, suppose we are given i.i.d. samples $\mathcal{D} = \{x_i\}_{i=1}^N$ from an unknown distribution $p_{\text{data}}$ over some space $\mathcal{X} \subseteq \mathbb{R}^d$. The generative modeling problem asks us to construct a generative model $(p_\theta, \mathsf{Sample}_\theta)$ consisting of two components: a parametric density $p_\theta: \mathcal{X} \to \mathbb{R}_+$ that approximates $p_{\text{data}}$, and a sampling procedure $\mathsf{Sample}_\theta$ that can efficiently draw samples $x \sim p_\theta$. The quality of our model is measured by how well $p_\theta$ approximates $p_{\text{data}}$ under some divergence measure $D(p_{\text{data}} \| p_\theta)$.

One common approach to obtaining $p_\theta$ is through Maximum Likelihood Estimation (MLE), which minimizes the Kullback-Leibler (KL) divergence—a measure of the difference between probability distributions. Specifically, MLE seeks parameters $\theta^\star \in \arg\min_\theta \mathrm{KL}(p_{\text{data}} \| p_\theta)$, which is equivalent to maximizing the expected log-likelihood: $\theta^\star \in \arg\min_\theta \mathbb{E}_{x \sim p_{\text{data}}}[-\log p_\theta(x)]$. In practice, we approximate this expectation using our finite dataset, leading to the empirical objective $\mathcal{L}_{\text{NLL}}(\theta) = -\frac{1}{N}\sum_{i=1}^N \log p_\theta(x_i)$, known as the negative log-likelihood. While MLE via KL minimization is the most common approach, alternative objectives exist, including the reverse KL divergence $\mathrm{KL}(p_\theta \| p_{\text{data}})$ which exhibits mode-seeking behavior, the symmetric Jensen-Shannon divergence $\mathrm{JS}(p_\theta, p_{\text{data}})$, and Wasserstein distances $W_p(p_\theta, p_{\text{data}})$ from optimal transport theory.

At the heart of generative modeling lies a fundamental computational trade-off, which we call the **sampling-likelihood duality**. For distributions over high-dimensional spaces where $d \gg 1$, there exists an inherent tension: if we can tractably evaluate the density $p_\theta(x)$ at arbitrary points, then efficiently drawing independent samples is often prohibitively expensive; conversely, if we can efficiently sample from $p_\theta$, then computing the exact likelihood $p_\theta(x)$ typically becomes intractable. This is not merely a limitation of current algorithms but rather a fundamental computational characteristic of high-dimensional probability distributions. Formally, the sampling mechanism $\mathsf{Sample}_\theta$ takes one of two forms: either a deterministic map $x = T_\theta(z)$ where $z \sim \mu$ is drawn from a simple base distribution, or the limit of a Markov chain $\lim_{k \to \infty} K_\theta^k(x_0)$ defined by a transition kernel $K_\theta: \mathcal{X} \to \mathcal{P}(\mathcal{X})$ with stationary distribution $p_\theta$. The choice between these mechanisms, and how we resolve the sampling-likelihood duality, fundamentally determines the structure of our generative model.

## 2. The Optimal Transport Perspective

The core insight that unifies modern generative modeling is this: **generative modeling is fundamentally about transporting probability mass from simple distributions to complex ones**. We begin with a simple base distribution $\mu$—typically a standard Gaussian $\mathcal{N}(0, I_d)$ or uniform distribution—where sampling is trivial and inexpensive. Our goal is to systematically transform or transport this simple distribution into the complex data distribution $p_{\text{data}}$, where sampling appears difficult but is achieved through this transformation.

This perspective is formalized through the concept of the **pushforward operator**. Given a measurable map $T: \mathcal{Z} \to \mathcal{X}$ and a probability measure $\mu \in \mathcal{P}(\mathcal{Z})$, the pushforward $T_\sharp \mu \in \mathcal{P}(\mathcal{X})$ is defined by $(T_\sharp \mu)(A) = \mu(T^{-1}(A))$ for all measurable sets $A \subseteq \mathcal{X}$. Intuitively, if we draw $Z \sim \mu$ and apply the transformation $X = T(Z)$, then $X$ has distribution $T_\sharp \mu$. With this notation, the generative modeling problem can be reformulated as: find a map $T_\theta: \mathcal{Z} \to \mathcal{X}$ such that the pushforward $T_{\theta^\star \sharp} \mu$ approximates $p_{\text{data}}$, where $\mu$ is our simple base distribution.

When the transport map $T$ is a smooth diffeomorphism—a bijection with smooth inverse—we can compute how densities transform under this map using the celebrated change of variables formula. Specifically, if $T: \mathcal{Z} \to \mathcal{X}$ is a $C^1$-diffeomorphism and $Z \sim \mu$ has density $\mu(z)$, then $X = T(Z)$ has density $p_X(x) = \mu(T^{-1}(x)) \cdot |\det J_{T^{-1}}(x)|$, where $J_{T^{-1}}(x) = \nabla_x T^{-1}(x) \in \mathbb{R}^{d \times d}$ is the Jacobian matrix of the inverse map. The Jacobian determinant $|\det J_{T^{-1}}(x)|$ quantifies how volumes change under the transformation $T$—it is the critical factor that accounts for how much the map stretches or compresses space at each point. In logarithmic form, this becomes $\log p_X(x) = \log \mu(z) + \log |\det J_{T^{-1}}(x)|$ where $z = T^{-1}(x)$, which will be central to understanding normalizing flows.

## 3. The Four Canonical Transport Mechanisms

All modern generative models can be understood as implementing one of four fundamental transport strategies, each resolving the sampling-likelihood duality in a different way. These four mechanisms—**Factorize**, **Transform**, **Evolve**, and **Shape**—represent distinct computational philosophies, each with characteristic training objectives, sampling procedures, and complexity trade-offs. 

To provide an overview, the **Factorize** mechanism uses triangular (Knothe-Rosenblatt) transport with $O(d)$ likelihood evaluation and $O(d)$ sequential sampling steps, making it memory-efficient but inherently serial. The **Transform** mechanism employs diffeomorphic maps, requiring either $O(d^3)$ computation for Jacobian determinants or ODE integration, but offering $O(1)$ one-shot sampling or efficient ODE-based generation; the trade-off is architectural constraints to maintain tractability. The **Evolve** mechanism uses stochastic paths with likelihood requiring ODE integration and sampling requiring $O(T)$ iterative refinement steps, leading to high computational cost but excellent sample quality. Finally, the **Shape** mechanism defines densities via energy functions and MCMC stationary distributions, with unnormalized likelihoods and sampling requiring $O(K)$ mixing time, often resulting in slow convergence but maximum modeling flexibility. Each mechanism represents a fundamentally different answer to the question: how do we transport probability mass from simple to complex distributions?

---

## 3.1 Mechanism I: Factorize

The first mechanism commits to decomposing the joint distribution through sequential conditioning. We impose an ordering on the dimensions $x = (x_1, \ldots, x_d)$ and apply the chain rule of probability to write $p_\theta(x) = \prod_{t=1}^d p_\theta(x_t \mid x_{<t})$, where $x_{<t} := (x_1, \ldots, x_{t-1})$ denotes all previous dimensions. This is called an **autoregressive decomposition**, and each conditional distribution $p_\theta(x_t \mid x_{<t})$ is parametrized by a neural network that takes the history $x_{<t}$ as input and outputs parameters for the conditional distribution over $x_t$. For continuous variables, this might be a Gaussian with learned mean and variance; for discrete tokens (like words), this is typically a categorical distribution over the vocabulary.

The beauty of this decomposition is that it gives us a concrete sampling procedure through what is known as **Knothe-Rosenblatt transport** or triangular transport. To generate a sample, we proceed sequentially: first sample $x_1$ from the marginal $p_\theta(x_1)$, then sample $x_2$ from the conditional $p_\theta(x_2 \mid x_1)$, then $x_3$ from $p_\theta(x_3 \mid x_1, x_2)$, and so on until we have generated all $d$ dimensions. More formally, we can express this as a deterministic transport from uniform random variables: define the cumulative distribution functions $F_t(x_t \mid x_{<t}) = \int_{-\infty}^{x_t} p_\theta(s \mid x_{<t})\, ds$, and construct the map $T_\theta: [0,1]^d \to \mathcal{X}$ given by $T_\theta(u) = (F_1^{-1}(u_1), F_2^{-1}(u_2 \mid x_1), \ldots, F_d^{-1}(u_d \mid x_{<d}))$. This map pushes the uniform distribution $\mathrm{Unif}[0,1]^d$ to $p_\theta$, and the triangular structure—where $x_t$ depends only on $u_t$ and $x_{<t}$—gives the mechanism its name. For discrete tokens, we simply replace continuous CDFs with categorical sampling: at each step $t$, we sample $x_t$ from the categorical distribution $\mathrm{Cat}(\pi_\theta(\cdot \mid x_{<t}))$ where $\pi_\theta(k \mid x_{<t}) = \mathbb{P}_\theta(x_t = k \mid x_{<t})$.

The factorized structure makes training remarkably clean. The log-likelihood is exact and fully tractable: $\log p_\theta(x) = \sum_{t=1}^d \log p_\theta(x_t \mid x_{<t})$, which means we can compute gradients without any approximation. The empirical training objective becomes $\mathcal{L}_{\text{AR}}(\theta) = -\frac{1}{N}\sum_{i=1}^N \sum_{t=1}^d \log p_\theta(x_i^{(t)} \mid x_i^{(<t)})$, which is simply the cross-entropy loss over all sequential predictions. This gives us exact negative log-likelihood for model selection, calibrated probabilities for uncertainty quantification, and stable training dynamics without adversarial objectives or mode collapse.

However, the factorization comes with inherent limitations. The computational profile reveals the core trade-off: while likelihood evaluation is $O(d)$ (one forward pass per dimension), sampling is also $O(d)$ but crucially **sequential**—we cannot parallelize across dimensions because each step depends on all previous steps. This sequential dependency creates latency in generation, particularly problematic for high-dimensional data like images (where $d \sim 10^6$ pixels) or long sequences (where $d \sim 10^3$ tokens). Additionally, autoregressive models suffer from **exposure bias**: during training they see ground-truth contexts $x_{<t}$, but during generation they must condition on their own predictions, leading to error accumulation when the model drifts into regions of the distribution it rarely saw during training. Finally, we must commit to a specific ordering of dimensions, and while theoretically any ordering defines a valid model, the choice can significantly impact both sample quality and training.