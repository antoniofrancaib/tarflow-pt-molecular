# =============================================================================
# ðŸŽ¯ MINIMALISTIC NORMALIZING FLOWS CONFIGURATION
# =============================================================================

# Default experiment settings
default:
  target: "two_moons"           # Options: two_moons, checkerboard
  base: "gaussian"              # Options: gaussian, gaussian_trainable
  model: "simple"               # Options: simple, transformer
  
  training:
    epochs: 3000
    batch_size: 512
    learning_rate: 1.0e-3
    eval_interval: 300
    
  model_params:
    hidden_dim: 128
    num_layers: 6
    
  visualization:
    domain: 3.5                 # Plot domain: [-domain, domain] x [-domain, domain]
    resolution: 100

# =============================================================================
# ðŸŽ¨ PRESET CONFIGURATIONS
# =============================================================================

presets:
  quick_test:
    target: "two_moons"
    model: "simple"
    training:
      epochs: 500
      batch_size: 256
    model_params:
      hidden_dim: 64
      num_layers: 4
      
  two_moons:
    target: "two_moons"
    model: "simple"
    training:
      epochs: 2000
      learning_rate: 5.0e-4
    model_params:
      hidden_dim: 64
      num_layers: 4
    visualization:
      domain: 4.0
      
  checkerboard:
    target: "checkerboard"
    model: "simple"
    training:
      epochs: 3000
      batch_size: 512
    model_params:
      hidden_dim: 128
      num_layers: 6
      
  checkerboard_hard:
    target: "checkerboard"
    model: "transformer"
    training:
      epochs: 4000
      learning_rate: 8.0e-4
    model_params:
      hidden_dim: 96
      num_layers: 8

# =============================================================================
# ðŸ”§ DISTRIBUTION PARAMETERS (Advanced Users Only)
# =============================================================================

distributions:
  checkerboard:
    grid_size: 8
    domain_size: 3.0
    high_density: 1.0
    low_density: 0.1
    
  transformer:
    transformer_layers: 2

# =============================================================================
# ðŸ§¬ MOLECULAR PARALLEL TEMPERING CONFIGURATIONS
# =============================================================================

molecular_pt:
  # Default 300K â†’ 450K transport
  aa_300_450:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0  # 300K
    target_temp_idx: 1  # 450K
    pdb_path: "datasets/AA/ref.pdb"
    
    model: "scalable_transformer"
    use_energy: true  # Use OpenMM energy evaluation
    
    training:
      epochs: 3000
      batch_size: 256
      learning_rate: 5.0e-4
      eval_interval: 100
      weight_decay: 1.0e-5
    
    model_params:
      num_flow_layers: 8
      embed_dim: 192
      num_heads: 8
      num_transformer_layers: 5
      dropout: 0.1
    
    normalization:
      mode: "per_atom"  # or "global"
  
  # GPU-optimized 300K â†’ 450K (for Colab/GPU with limited VRAM)
  aa_300_450_gpu:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0  # 300K
    target_temp_idx: 1  # 450K
    pdb_path: "datasets/AA/ref.pdb"
    
    model: "scalable_transformer"
    use_energy: true
    
    training:
      epochs: 3000
      batch_size: 32  # Reduced for GPU memory
      learning_rate: 5.0e-4
      eval_interval: 100
      weight_decay: 1.0e-5
    
    model_params:
      num_flow_layers: 4  # Reduced from 8
      embed_dim: 96      # Reduced from 192
      num_heads: 4       # Reduced from 8
      num_transformer_layers: 3  # Reduced from 5
      dropout: 0.1
    
    normalization:
      mode: "per_atom"
    
  # Higher temperature gap: 300K â†’ 670K
  aa_300_670:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0  # 300K
    target_temp_idx: 2  # 670K
    pdb_path: "datasets/AA/ref.pdb"
    
    model: "scalable_transformer"
    use_energy: true
    
    training:
      epochs: 5000
      batch_size: 256
      learning_rate: 3.0e-4
      eval_interval: 150
      weight_decay: 1.0e-5
    
    model_params:
      num_flow_layers: 10
      embed_dim: 256
      num_heads: 8
      num_transformer_layers: 6
      dropout: 0.15
    
    normalization:
      mode: "per_atom"
  
  # Maximum gap: 300K â†’ 1000K
  aa_300_1000:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0  # 300K
    target_temp_idx: 3  # 1000K
    pdb_path: "datasets/AA/ref.pdb"
    
    model: "scalable_transformer"
    use_energy: true
    
    training:
      epochs: 6000
      batch_size: 256
      learning_rate: 2.0e-4
      eval_interval: 200
      weight_decay: 1.0e-5
    
    model_params:
      num_flow_layers: 12
      embed_dim: 320
      num_heads: 8
      num_transformer_layers: 8
      dropout: 0.2
    
    normalization:
      mode: "per_atom"

# Temperature ladder for PT
temperatures:
  values: [300.0, 450.0, 670.0, 1000.0]