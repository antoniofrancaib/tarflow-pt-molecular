# =============================================================================
# ðŸŽ¯ MINIMALISTIC NORMALIZING FLOWS CONFIGURATION
# =============================================================================

# Default experiment settings
default:
  target: "two_moons"           # Options: two_moons, checkerboard
  base: "gaussian"              # Options: gaussian, gaussian_trainable
  model: "simple"               # Options: simple, transformer
  
  training:
    epochs: 3000
    batch_size: 512
    learning_rate: 1.0e-3
    eval_interval: 300
    
  model_params:
    hidden_dim: 128
    num_layers: 6
    
  visualization:
    domain: 3.5                 # Plot domain: [-domain, domain] x [-domain, domain]
    resolution: 100

# =============================================================================
# ðŸŽ¨ PRESET CONFIGURATIONS
# =============================================================================

presets:
  quick_test:
    target: "two_moons"
    model: "simple"
    training:
      epochs: 500
      batch_size: 256
    model_params:
      hidden_dim: 64
      num_layers: 4
      
  two_moons:
    target: "two_moons"
    model: "simple"
    training:
      epochs: 2000
      learning_rate: 5.0e-4
    model_params:
      hidden_dim: 64
      num_layers: 4
    visualization:
      domain: 4.0
      
  checkerboard:
    target: "checkerboard"
    model: "simple"
    training:
      epochs: 3000
      batch_size: 512
    model_params:
      hidden_dim: 128
      num_layers: 6
      
  checkerboard_hard:
    target: "checkerboard"
    model: "transformer"
    training:
      epochs: 4000
      learning_rate: 8.0e-4
    model_params:
      hidden_dim: 96
      num_layers: 8

# =============================================================================
# ðŸ”§ DISTRIBUTION PARAMETERS (Advanced Users Only)
# =============================================================================

distributions:
  checkerboard:
    grid_size: 8
    domain_size: 3.0
    high_density: 1.0
    low_density: 0.1
    
  transformer:
    transformer_layers: 2

# =============================================================================
# ðŸ§¬ MOLECULAR PARALLEL TEMPERING CONFIGURATIONS
# =============================================================================

molecular_pt:
  # Default 300K â†’ 450K transport (CPU/local with larger model)
  aa_300_450:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0  # 300K
    target_temp_idx: 1  # 450K
    pdb_path: "datasets/AA/ref.pdb"
    
    model: "scalable_transformer"
    use_energy: true
    use_energy_gating: true
    E_cut: 500.0
    E_max: 100000000.0
    logdet_clip: 100.0
    
    training:
      epochs: 3000
      batch_size: 256
      learning_rate: 1.0e-4
      eval_interval: 100
      weight_decay: 1.0e-5
      use_warmup: true
      warmup_epochs: 50
      use_scheduler: true
      scheduler_patience: 50
      scheduler_factor: 0.5
      save_best_model: true
      gradient_clip_norm: 0.5
    
    model_params:
      num_flow_layers: 8
      embed_dim: 192
      num_heads: 8
      num_transformer_layers: 5
      dropout: 0.1
    
    normalization:
      mode: "per_atom"  # or "global"
  
  # GPU-optimized 300K â†’ 450K (for Colab/GPU with limited VRAM)
  # Balanced: ~6.5M parameters (vs 1.3M too small, 17.8M too large for T4)
  aa_300_450_gpu:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0  # 300K
    target_temp_idx: 1  # 450K
    pdb_path: "datasets/AA/ref.pdb"
    
    model: "scalable_transformer"
    use_energy: true
    
    # Energy gating (for ablation studies)
    use_energy_gating: true  # ABLATION: Set false to disable
    E_cut: 500.0  # Soft regularization threshold (kJ/mol)
    E_max: 100000000.0  # Hard clamp threshold after warmup (100M kJ/mol)
    logdet_clip: 100.0  # Clamp LogDet to prevent divergence
    
    training:
      epochs: 3000
      batch_size: 32  # Reduced for memory safety on T4
      learning_rate: 1.0e-4  # Reduced from 5e-4 to prevent divergence
      eval_interval: 100
      weight_decay: 1.0e-5
      
      # Learning rate scheduling (for ablation studies)
      use_warmup: true  # ABLATION: Set false to disable LR warmup
      warmup_epochs: 50  # Number of warmup epochs
      use_scheduler: true  # ABLATION: Set false for constant LR
      scheduler_patience: 50  # Epochs before LR reduction
      scheduler_factor: 0.5  # LR reduction factor
      
      # Checkpointing (for ablation studies)
      save_best_model: true  # ABLATION: Set false to only save final model
      gradient_clip_norm: 0.5  # Max gradient norm (reduced for stability)
    
    model_params:
      num_flow_layers: 6  # Increased from 4 (was too small)
      embed_dim: 128      # Increased from 96
      num_heads: 8        # Increased from 4 (matches standard)
      num_transformer_layers: 4  # Increased from 3
      dropout: 0.1
    
    normalization:
      mode: "per_atom"
  
  # Ultra-light GPU preset (for very limited VRAM)
  aa_300_450_gpu_light:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0  # 300K
    target_temp_idx: 1  # 450K
    pdb_path: "datasets/AA/ref.pdb"
    
    model: "scalable_transformer"
    use_energy: true
    use_energy_gating: true
    E_cut: 500.0
    E_max: 100000000.0  # 100M kJ/mol for stability
    
    training:
      epochs: 3000
      batch_size: 16  # Very small for memory-constrained GPUs
      learning_rate: 5.0e-4
      eval_interval: 100
      weight_decay: 1.0e-5
      use_warmup: true
      warmup_epochs: 50
      use_scheduler: true
      scheduler_patience: 50
      scheduler_factor: 0.5
      save_best_model: true
      gradient_clip_norm: 1.0
    
    model_params:
      num_flow_layers: 4  # Smaller model
      embed_dim: 96       # Smaller embeddings
      num_heads: 4        # Fewer heads
      num_transformer_layers: 3  # Fewer layers
      dropout: 0.1
    
    normalization:
      mode: "per_atom"
    
  # Higher temperature gap: 300K â†’ 670K
  aa_300_670:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0  # 300K
    target_temp_idx: 2  # 670K
    pdb_path: "datasets/AA/ref.pdb"
    
    model: "scalable_transformer"
    use_energy: true
    
    training:
      epochs: 5000
      batch_size: 256
      learning_rate: 3.0e-4
      eval_interval: 150
      weight_decay: 1.0e-5
    
    model_params:
      num_flow_layers: 10
      embed_dim: 256
      num_heads: 8
      num_transformer_layers: 6
      dropout: 0.15
    
    normalization:
      mode: "per_atom"
  
  # Maximum gap: 300K â†’ 1000K
  aa_300_1000:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0  # 300K
    target_temp_idx: 3  # 1000K
    pdb_path: "datasets/AA/ref.pdb"
    
    model: "scalable_transformer"
    use_energy: true
    
    training:
      epochs: 6000
      batch_size: 256
      learning_rate: 2.0e-4
      eval_interval: 200
      weight_decay: 1.0e-5
    
    model_params:
      num_flow_layers: 12
      embed_dim: 320
      num_heads: 8
      num_transformer_layers: 8
      dropout: 0.2
    
    normalization:
      mode: "per_atom"

# Temperature ladder for PT
temperatures:
  values: [300.0, 450.0, 670.0, 1000.0]

# =============================================================================
# ðŸ”¬ ABLATION STUDY PRESETS
# =============================================================================
# Use these to systematically test which features help training

ablation_studies:
  # Baseline: ALL features enabled (best performance)
  baseline_gpu:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0
    target_temp_idx: 1
    pdb_path: "datasets/AA/ref.pdb"
    model: "scalable_transformer"
    use_energy: true
    use_energy_gating: true
    E_cut: 500.0
    E_max: 100000000.0  # 100M kJ/mol
    training:
      epochs: 1500
      batch_size: 32  # Reduced for GPU memory safety
      learning_rate: 5.0e-4
      eval_interval: 100
      weight_decay: 1.0e-5
      use_warmup: true
      warmup_epochs: 50
      use_scheduler: true
      scheduler_patience: 50
      scheduler_factor: 0.5
      save_best_model: true
      gradient_clip_norm: 1.0
    model_params:
      num_flow_layers: 6
      embed_dim: 128
      num_heads: 8
      num_transformer_layers: 4
      dropout: 0.1
    normalization:
      mode: "per_atom"
  
  # Ablation 1: No energy gating (test if gating helps)
  no_energy_gating_gpu:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0
    target_temp_idx: 1
    pdb_path: "datasets/AA/ref.pdb"
    model: "scalable_transformer"
    use_energy: true
    use_energy_gating: false  # ABLATED
    training:
      epochs: 1500
      batch_size: 32  # Reduced for GPU memory safety
      learning_rate: 5.0e-4
      eval_interval: 100
      weight_decay: 1.0e-5
      use_warmup: true
      warmup_epochs: 50
      use_scheduler: true
      scheduler_patience: 50
      scheduler_factor: 0.5
      save_best_model: true
      gradient_clip_norm: 1.0
    model_params:
      num_flow_layers: 6
      embed_dim: 128
      num_heads: 8
      num_transformer_layers: 4
      dropout: 0.1
    normalization:
      mode: "per_atom"
  
  # Ablation 2: No warmup (test if warmup helps)
  no_warmup_gpu:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0
    target_temp_idx: 1
    pdb_path: "datasets/AA/ref.pdb"
    model: "scalable_transformer"
    use_energy: true
    use_energy_gating: true
    E_cut: 500.0
    E_max: 100000000.0  # 100M kJ/mol
    training:
      epochs: 1500
      batch_size: 32  # Reduced for GPU memory safety
      learning_rate: 5.0e-4
      eval_interval: 100
      weight_decay: 1.0e-5
      use_warmup: false  # ABLATED
      use_scheduler: true
      scheduler_patience: 50
      scheduler_factor: 0.5
      save_best_model: true
      gradient_clip_norm: 1.0
    model_params:
      num_flow_layers: 6
      embed_dim: 128
      num_heads: 8
      num_transformer_layers: 4
      dropout: 0.1
    normalization:
      mode: "per_atom"
  
  # Ablation 3: No scheduler (constant LR)
  no_scheduler_gpu:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0
    target_temp_idx: 1
    pdb_path: "datasets/AA/ref.pdb"
    model: "scalable_transformer"
    use_energy: true
    use_energy_gating: true
    E_cut: 500.0
    E_max: 100000000.0  # 100M kJ/mol
    training:
      epochs: 1500
      batch_size: 32  # Reduced for GPU memory safety
      learning_rate: 5.0e-4
      eval_interval: 100
      weight_decay: 1.0e-5
      use_warmup: true
      warmup_epochs: 50
      use_scheduler: false  # ABLATED
      save_best_model: true
      gradient_clip_norm: 1.0
    model_params:
      num_flow_layers: 6
      embed_dim: 128
      num_heads: 8
      num_transformer_layers: 4
      dropout: 0.1
    normalization:
      mode: "per_atom"
  
  # Ablation 4: No gradient clipping
  no_grad_clip_gpu:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0
    target_temp_idx: 1
    pdb_path: "datasets/AA/ref.pdb"
    model: "scalable_transformer"
    use_energy: true
    use_energy_gating: true
    E_cut: 500.0
    E_max: 100000000.0  # 100M kJ/mol
    training:
      epochs: 1500
      batch_size: 32  # Reduced for GPU memory safety
      learning_rate: 5.0e-4
      eval_interval: 100
      weight_decay: 1.0e-5
      use_warmup: true
      warmup_epochs: 50
      use_scheduler: true
      scheduler_patience: 50
      scheduler_factor: 0.5
      save_best_model: true
      gradient_clip_norm: 0.0  # ABLATED
    model_params:
      num_flow_layers: 6
      embed_dim: 128
      num_heads: 8
      num_transformer_layers: 4
      dropout: 0.1
    normalization:
      mode: "per_atom"
  
  # Ablation 5: Minimal (everything disabled except basics)
  minimal_gpu:
    data_path: "datasets/AA/pt_AA.pt"
    source_temp_idx: 0
    target_temp_idx: 1
    pdb_path: "datasets/AA/ref.pdb"
    model: "scalable_transformer"
    use_energy: true
    use_energy_gating: false  # ABLATED
    training:
      epochs: 1500
      batch_size: 32  # Reduced for GPU memory safety
      learning_rate: 5.0e-4
      eval_interval: 100
      weight_decay: 1.0e-5
      use_warmup: false  # ABLATED
      use_scheduler: false  # ABLATED
      save_best_model: false  # ABLATED
      gradient_clip_norm: 0.0  # ABLATED
    model_params:
      num_flow_layers: 6
      embed_dim: 128
      num_heads: 8
      num_transformer_layers: 4
      dropout: 0.1
    normalization:
      mode: "per_atom"